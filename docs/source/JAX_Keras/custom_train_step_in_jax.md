# JAXë¡œ `fit()`ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ ì‚¬ìš©ì ì •ì˜í•˜ê¸°(Customizing what happens in fit() with JAX)

**ì €ì:** [fchollet](https://twitter.com/fchollet)<br>
**ì—­ì:** [ì¡°í˜„ì„](mailto:hoyajigi@gmail.com)<br>
**ê²€ìˆ˜:** ì´ì˜ë¹ˆ, ë°•ì •í˜„<br>
**ìƒì„± ë‚ ì§œ:** 2023/06/27<br>
**ë§ˆì§€ë§‰ ìˆ˜ì •:** 2023/06/27<br>
**ì„¤ëª…:** ëª¨ë¸ í´ë˜ìŠ¤ì˜ í›ˆë ¨ ë‹¨ê³„ë¥¼ JAXë¡œ ì¬ì •ì˜í•©ë‹ˆë‹¤.

<a href="https://colab.research.google.com/drive/1sSz6_fi8S0OHn3T_73046sI2P1rB4-xy" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"/></a>


## ì†Œê°œ

ì§€ë„ í•™ìŠµì„ í•  ë•Œ `fit()`ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë“  ê²ƒì´ ì›í™œí•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.

ëª¨ë“  ì„¸ë¶€ ì‚¬í•­ì„ ì œì–´í•´ì•¼ í•˜ëŠ” ê²½ìš°, ìì‹ ë§Œì˜ í›ˆë ¨
ë£¨í”„ë¥¼ ì™„ì „íˆ ì²˜ìŒë¶€í„° ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ì‚¬ìš©ì ì§€ì • í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ì „íˆ
ì½œë°±, ê¸°ë³¸ ì œê³µ ë°°í¬ ì§€ì›ê³¼ ê°™ì€ `fit()`ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ê³  ì‹¶ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
ë˜ëŠ” ë‹¨ê³„ ìœµí•©ê³¼ ê°™ì€ í¸ë¦¬í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?

Kerasì˜ í•µì‹¬ ì›ì¹™ì€ **ë³µì¡ì„±ì˜ ì ì§„ì  ê³µê°œ**ì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ”
í•­ìƒ ì ì§„ì ì¸ ë°©ì‹ìœ¼ë¡œ ë¡œìš° ë ˆë²¨ì˜ ì›Œí¬í”Œë¡œìš°ì— ì§„ì…í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
í•˜ì´ ë ˆë²¨ì˜ ê¸°ëŠ¥ì´ ì‚¬ìš© ì‚¬ë¡€ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•´ì„œ ê°‘ìê¸° ë¡œìš° ë ˆë²¨ë¡œ ë°”ë€Œë©´ ì•ˆ ë©ë‹ˆë‹¤. ë†’ì€ ìˆ˜ì¤€ì˜ í¸ì˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì‘ì€ ì„¸ë¶€ ì‚¬í•­ì„ ë” ì˜ ì œì–´í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

`fit()`ì˜ ê¸°ëŠ¥ì„ ì‚¬ìš©ì ì •ì˜í•´ì•¼ í•˜ëŠ” ê²½ìš°, `Model` í´ë˜ìŠ¤ì˜ í›ˆë ¨ ë‹¨ê³„ í•¨ìˆ˜ë¥¼ **ì¬ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤**. ì´ í•¨ìˆ˜ëŠ” ëª¨ë“  ë°ì´í„° ë°°ì¹˜ì— ëŒ€í•´ `fit()`ì— ì˜í•´ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ í‰ì†Œì²˜ëŸ¼ `fit()`ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìœ¼ë©°, ìì²´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

ì´ íŒ¨í„´ì€ í•¨ìˆ˜í˜• APIë¡œ ëª¨ë¸ì„ ë¹Œë“œí•˜ëŠ” ê²ƒì„ ë°©í•´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'ì‹œí€€ì…œ' ëª¨ë¸, í•¨ìˆ˜í˜• API ëª¨ë¸ ë˜ëŠ” í•˜ìœ„ í´ë˜ìŠ¤ ëª¨ë¸ì„ ë¹Œë“œí•˜ë“  ìƒê´€ì—†ì´ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## ì„¤ì •


```python
import os

# ì´ ê°€ì´ë“œëŠ” JAX ë°±ì—”ë“œì—ì„œë§Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
os.environ["KERAS_BACKEND"] = "jax"

import jax
import keras_core as keras
import numpy as np
```

    Using JAX backend.


## ì²« ë²ˆì§¸ ê°„ë‹¨í•œ ì˜ˆì‹œ

ê°„ë‹¨í•œ ì˜ˆì œë¶€í„° ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤:

- ìš°ë¦¬ëŠ” `keras.Model`ì„ ìƒì†í•˜ëŠ” ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ëª¨ë¸ì˜ ë¹„í•™ìŠµ ê°€ëŠ¥ ë³€ìˆ˜ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ëœ ê°’ê³¼ ì†ì‹¤ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì™„ì „ ìŠ¤í…Œì´íŠ¸ë¦¬ìŠ¤ `compute_loss_and_updates()` ë©”ì„œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œëŠ” `stateless_call()`ê³¼ ë‚´ì¥ëœ `compute_loss()`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
- ì™„ì „ ìŠ¤í…Œì´íŠ¸ë¦¬ìŠ¤ `train_step()` ë©”ì„œë“œë¥¼ êµ¬í˜„í•˜ì—¬ í˜„ì¬ ë©”íŠ¸ë¦­ ê°’(ì†ì‹¤ í¬í•¨)ê³¼ í•™ìŠµ ê°€ëŠ¥í•œ ë³€ìˆ˜, ì˜µí‹°ë§ˆì´ì € ë³€ìˆ˜, ë©”íŠ¸ë¦­ ë³€ìˆ˜ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ëœ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

ì°¸ê³ ë¡œ `sample_weight` ì¸ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê³ ë ¤í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

- ë°ì´í„°ë¥¼ `x, y, sample_weight = data`ë¡œ ì–¸íŒ¨í‚¹í•©ë‹ˆë‹¤.
- sample_weight`ë¥¼ `compute_loss()`ì— ì „ë‹¬í•©ë‹ˆë‹¤.
- sample_weight`ë¥¼ `y` ë° `y_pred`ì™€ í•¨ê»˜ ì „ë‹¬í•©ë‹ˆë‹¤.
ì™€ í•¨ê»˜ `stateless_update_state()`ì˜ ë©”íŠ¸ë¦­ì— ì „ë‹¬í•˜ê¸°


```python
class CustomModel(keras.Model):
    def compute_loss_and_updates(
        self,
        trainable_variables,
        non_trainable_variables,
        x,
        y,
        training=False,
    ):
        y_pred, non_trainable_variables = self.stateless_call(
            trainable_variables,
            non_trainable_variables,
            x,
            training=training,
        )
        loss = self.compute_loss(x, y, y_pred)
        return loss, (y_pred, non_trainable_variables)

    def train_step(self, state, data):
        (
            trainable_variables,
            non_trainable_variables,
            optimizer_variables,
            metrics_variables,
        ) = state
        x, y = data

        # ê·¸ë¼ë°ì´ì…˜ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        grad_fn = jax.value_and_grad(self.compute_loss_and_updates, has_aux=True)

        # ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        (loss, (y_pred, non_trainable_variables)), grads = grad_fn(
            trainable_variables,
            non_trainable_variables,
            x,
            y,
            training=True,
        )

        # í•™ìŠµ ê°€ëŠ¥í•œ ë³€ìˆ˜ ë° ìµœì í™” ë³€ìˆ˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        (
            trainable_variables,
            optimizer_variables,
        ) = self.optimizer.stateless_apply(
            optimizer_variables, grads, trainable_variables
        )

        # ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        new_metrics_vars = []
        for metric in self.metrics:
            this_metric_vars = metrics_variables[
                len(new_metrics_vars) : len(new_metrics_vars) + len(metric.variables)
            ]
            if metric.name == "loss":
                this_metric_vars = metric.stateless_update_state(this_metric_vars, loss)
            else:
                this_metric_vars = metric.stateless_update_state(
                    this_metric_vars, y, y_pred
                )
            logs = metric.stateless_result(this_metric_vars)
            new_metrics_vars += this_metric_vars

        # ë©”íŠ¸ë¦­ ë¡œê·¸ì™€ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë³€ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        state = (
            trainable_variables,
            non_trainable_variables,
            optimizer_variables,
            new_metrics_vars,
        )
        return logs, state

```

í•œë²ˆ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:


```python
# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)
model.compile(optimizer="adam", loss="mse", metrics=["mae"])

# í‰ì†Œì²˜ëŸ¼ 'fit'ì„ ì‚¬ìš©í•˜ì„¸ìš”.
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
model.fit(x, y, epochs=3)

```

    Epoch 1/3
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - mae: 0.4484 - loss: 0.2870
    Epoch 2/3
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 1ms/step - mae: 0.4020 - loss: 0.2704
    Epoch 3/3
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 1ms/step - mae: 0.4182 - loss: 0.2542





    <keras_core.src.callbacks.history.History at 0x7be1c8068400>



## ë¡œìš° ë ˆë²¨ë¡œ í•´ë³´ê¸°

ë‹¹ì—°íˆ `compile()`ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ê±´ë„ˆë›°ê³  ëŒ€ì‹  `train_step`ì—ì„œ
ëª¨ë“  ê²ƒì„ *ìˆ˜ë™ìœ¼ë¡œ* í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë©”íŠ¸ë¦­ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ `compile()`ë§Œ ì‚¬ìš©í•˜ì—¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ êµ¬ì„±í•˜ëŠ” ë¡œìš° ë ˆë²¨ì˜ ì˜ˆì œì…ë‹ˆë‹¤:


```python
class CustomModel(keras.Model):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.mae_metric = keras.metrics.MeanAbsoluteError(name="mae")
        self.loss_fn = keras.losses.MeanSquaredError()

    def compute_loss_and_updates(
        self,
        trainable_variables,
        non_trainable_variables,
        x,
        y,
        training=False,
    ):
        y_pred, non_trainable_variables = self.stateless_call(
            trainable_variables,
            non_trainable_variables,
            x,
            training=training,
        )
        loss = self.loss_fn(y, y_pred)
        return loss, (y_pred, non_trainable_variables)

    def train_step(self, state, data):
        (
            trainable_variables,
            non_trainable_variables,
            optimizer_variables,
            metrics_variables,
        ) = state
        x, y = data

        # ê·¸ë¼ë°ì´ì…˜ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        grad_fn = jax.value_and_grad(self.compute_loss_and_updates, has_aux=True)

        # ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        (loss, (y_pred, non_trainable_variables)), grads = grad_fn(
            trainable_variables,
            non_trainable_variables,
            x,
            y,
            training=True,
        )

        # í•™ìŠµ ê°€ëŠ¥í•œ ë³€ìˆ˜ ë° ìµœì í™” ë³€ìˆ˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        (
            trainable_variables,
            optimizer_variables,
        ) = self.optimizer.stateless_apply(
            optimizer_variables, grads, trainable_variables
        )

        # ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        loss_tracker_vars = metrics_variables[: len(self.loss_tracker.variables)]
        mae_metric_vars = metrics_variables[len(self.loss_tracker.variables) :]

        loss_tracker_vars = self.loss_tracker.stateless_update_state(
            loss_tracker_vars, loss
        )
        mae_metric_vars = self.mae_metric.stateless_update_state(
            mae_metric_vars, y, y_pred
        )

        logs = {}
        logs[self.loss_tracker.name] = self.loss_tracker.stateless_result(
            loss_tracker_vars
        )
        logs[self.mae_metric.name] = self.mae_metric.stateless_result(mae_metric_vars)

        new_metrics_vars = loss_tracker_vars + mae_metric_vars

        # ë©”íŠ¸ë¦­ ë¡œê·¸ì™€ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë³€ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        state = (
            trainable_variables,
            non_trainable_variables,
            optimizer_variables,
            new_metrics_vars,
        )
        return logs, state

    @property
    def metrics(self):
        # ì—¬ê¸°ì— `Metric` ê°ì²´ë¥¼ ë‚˜ì—´í•˜ì—¬ `reset_states()`ê°€
        # ê° ì—í¬í¬ê°€ ì‹œì‘ë  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ê±°ë‚˜
        # ë˜ëŠ” `evaluate()`ê°€ ì‹œì‘ë  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        return [self.loss_tracker, self.mae_metric]


# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)

# ì—¬ê¸°ì„œëŠ” ì†ì‹¤ì´ë‚˜ ì§€í‘œë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
model.compile(optimizer="adam")

# í‰ì†Œì²˜ëŸ¼ `fit`ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. ì½œë°± ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
model.fit(x, y, epochs=5)

```

    Epoch 1/5
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.3712 - mae: 0.4860
    Epoch 2/5
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.2637 - mae: 0.4173
    Epoch 3/5
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.2385 - mae: 0.4012
    Epoch 4/5
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.2407 - mae: 0.3952
    Epoch 5/5
    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - loss: 0.2145 - mae: 0.3782





    <keras_core.src.callbacks.history.History at 0x7be1b02f3a90>



## ìì²´ í‰ê°€ ë‹¨ê³„ ì œê³µ

`model.evaluate()` í˜¸ì¶œì— ëŒ€í•´ ë™ì¼í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? ê·¸ë ‡ë‹¤ë©´
test_step`ì„ ì •í™•íˆ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì¬ì •ì˜í•˜ë©´ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë©ë‹ˆë‹¤:


```python
class CustomModel(keras.Model):
    def test_step(self, state, data):
        # ë°ì´í„° ì–¸íŒ©í‚¹í•©ë‹ˆë‹¤.
        x, y = data
        (
            trainable_variables,
            non_trainable_variables,
            metrics_variables,
        ) = state

        # ì˜ˆì¸¡ê³¼ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        y_pred, non_trainable_variables = self.stateless_call(
            trainable_variables,
            non_trainable_variables,
            x,
            training=False,
        )
        loss = self.compute_loss(x, y, y_pred)

        # ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        new_metrics_vars = []
        for metric in self.metrics:
            this_metric_vars = metrics_variables[
                len(new_metrics_vars) : len(new_metrics_vars) + len(metric.variables)
            ]
            if metric.name == "loss":
                this_metric_vars = metric.stateless_update_state(this_metric_vars, loss)
            else:
                this_metric_vars = metric.stateless_update_state(
                    this_metric_vars, y, y_pred
                )
            logs = metric.stateless_result(this_metric_vars)
            new_metrics_vars += this_metric_vars

        # ë©”íŠ¸ë¦­ ë¡œê·¸ì™€ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë³€ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        state = (
            trainable_variables,
            non_trainable_variables,
            new_metrics_vars,
        )
        return logs, state


# CustomModelì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)
model.compile(loss="mse", metrics=["mae"])

# ì‚¬ìš©ì ì •ì˜ test_stepìœ¼ë¡œ í‰ê°€í•˜ê¸°
x = np.random.random((1000, 32))
y = np.random.random((1000, 1))
model.evaluate(x, y)

```

    [1m32/32[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 6ms/step - mae: 0.6693 - loss: 0.6276





    [0.6276098489761353, 0.6762693524360657]



ì´ê²Œ ì „ë¶€ì…ë‹ˆë‹¤!
