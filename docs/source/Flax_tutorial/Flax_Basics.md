# Flax ê¸°ì´ˆ

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1b35Yrx7E79qWuhovex-cL7g5TIUYVzju?usp=sharing)

[![Open On GitHub](https://img.shields.io/badge/Open-on%20GitHub-blue?logo=GitHub)](https://github.com/google/flax/blob/main/docs/notebooks/state_params.ipynb)


ë²ˆì—­: ì¥ì§„ìš° [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white
)](www.linkedin.com/in/jinwoo1126) \\

í•´ë‹¹ ë…¸íŠ¸ë¶ì€ ì•„ë˜ì˜ íë¦„ì— ë”°ë¼ ì—¬ëŸ¬ë¶„ë“¤ê»˜ Flaxë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.

- Flax ë‚´ì¥ ë ˆì´ì–´ ë˜ëŠ” third-party ëª¨ë¸ë¡œë¶€í„° ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™” í•˜ëŠ” ë°©ë²•.
- ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ì™€ ìˆ˜ë™ìœ¼ë¡œ ì‘ì„±ëœ í›ˆë ¨ì„ ì´ˆê¸°í™” í•˜ëŠ” ë°©ë²•.
- Flaxì—ì„œ ì œê³µí•˜ëŠ” optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ë°©ë²•.
- íŒŒë¼ë¯¸í„°ë“¤ê³¼ ë‹¤ë¥¸ ê°ì²´ë“¤ì„ ì§ë ¬í™”í•˜ëŠ” ë°©ë²•.
- ìì²´ ëª¨ë¸ì„ ë§Œë“¤ê³  ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ë²•.

## í™˜ê²½ ì„¤ì • ë°©ë²•

ë‹¤ìŒì€ í•´ë‹¹ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”í•œ í™˜ê²½ ì„¤ì • ì½”ë“œì…ë‹ˆë‹¤.


```python
# ìµœì‹  JAXlib version ì„¤ì¹˜.
!pip install --upgrade -q pip jax jaxlib
# Flax ì„¤ì¹˜:
!pip install --upgrade -q git+https://github.com/google/flax.git
```

    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.3/1.3 MB[0m [31m49.1 MB/s[0m eta [36m0:00:00[0m
    [?25h  Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
      Preparing metadata (pyproject.toml) ... [?25l[?25hdone
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m71.6/71.6 MB[0m [31m13.5 MB/s[0m eta [36m0:00:00[0m
    [?25h  Building wheel for jax (pyproject.toml) ... [?25l[?25hdone
      Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
      Installing backend dependencies ... [?25l[?25hdone
      Preparing metadata (pyproject.toml) ... [?25l[?25hdone
      Building wheel for flax (pyproject.toml) ... [?25l[?25hdone



```python
import jax
from typing import Any, Callable, Sequence
from jax import lax, random, numpy as jnp
from flax.core import freeze, unfreeze
from flax import linen as nn
```

## Flaxë¥¼ ì´ìš©í•œ ì„ í˜• íšŒê·€

ì´ì „ì˜ JAX ë…¸íŠ¸ë¶ì—ì„œëŠ” ì„ í˜• íšŒê·€ì— ëŒ€í•œ ì‹¤ìŠµì„ ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤. ì•„ì‹œë‹¤ì‹œí”¼, ì„ í˜• íšŒê·€ëŠ” í•˜ë‚˜ì˜ dense neural network layerë¥¼ ì´ìš©í•´ì„œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì—ëŠ” ì´ì— í•´ë‹¹í•˜ëŠ” ì˜ˆì œë¥¼ ë³´ê³  ì–´ë–»ê²Œ ë™ì‘ì´ ìˆ˜í–‰ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

ì´ í•˜ë‚˜ì˜ dense layerëŠ” kernel parameter $W \in M_{m,n}(R)$ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ì»¤ë„ì€ nì°¨ì›ì˜ inputê³¼ ëª¨ë¸ì˜ ì¶œë ¥ì´ ë˜ëŠ” mì°¨ì›ì˜ featureì™€ mì°¨ì›ì˜ bias parameter $b \in R^m$ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ dense layerëŠ” ì…ë ¥ ê°’ $x \in R^n$ìœ¼ë¡œë¶€í„° $Wx+b$ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

í•´ë‹¹ dense layerëŠ” ì´ë¯¸ Flaxì˜ `flax.linen` ëª¨ë“ˆì—ì„œ ì œê³µë˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.


```python
# í•˜ë‚˜ì˜ dense layer ìƒì„± ('features'ë¥¼ ì…ë ¥ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§ )
model = nn.Dense(features=5)
```

ì¼ë°˜ì ì¸ ë ˆì´ì–´ë“¤ì€ linen.Module í´ë˜ìŠ¤ì˜ ì„œë¸Œ í´ë˜ìŠ¤ì— ìˆìŠµë‹ˆë‹¤.

## ëª¨ë¸ íŒŒë¼ë¯¸í„° & ì´ˆê¸°í™”

íŒŒë¼ë¯¸í„° ê°’ë“¤ì€ ëª¨ë¸ ìì²´ì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, PRNGì™€ ë”ë¯¸ ì…ë ¥ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ `init`í•¨ìˆ˜ë¥¼ í†µí•´ ì´ˆê¸°í™”í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.


```python
key1, key2 = random.split(random.PRNGKey(0))
x = random.normal(key1, (10,)) # ë”ë¯¸ ì…ë ¥ ë°ì´í„°
params = model.init(key2, x) # ì´ˆê¸°í™” í˜¸ì¶œ
jax.tree_util.tree_map(lambda x: x.shape, params) # ì¶œë ¥ í˜•íƒœ í™•ì¸
```

    WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)





    FrozenDict({
        params: {
            bias: (5,),
            kernel: (10, 5),
        },
    })



*ì£¼ì˜ : JAXì™€ FlaxëŠ” NumPyì™€ ê°™ì´ row-based ì‹œìŠ¤í…œì„ ë”°ë¦…ë‹ˆë‹¤. ì¦‰, ë²¡í„°ë“¤ì´ columne ë²¡í„°ê°€ ì•„ë‹Œ row ë²¡í„°ë¡œ í‘œí˜„ëœë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì€ ì»¤ë„ì˜ shapeì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ( params â†’ kernel : (10, 5) )*

ê²°ê³¼ëŠ” ì˜ˆìƒí•œëŒ€ë¡œ kernelê³¼ biasì˜ í¬ê¸°ê°€ ê°™ì€ ì‚¬ì´ì¦ˆë¡œ ìƒì„±ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ë¶€ ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- ë”ë¯¸ ì…ë ¥ ë°ì´í„° `x`ëŠ” shapeì˜ ì¶”ë¡ ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëª¨ë¸ì„ ì„ ì–¸í•  ë•Œ ëª¨ë¸ì˜ ì¶œë ¥ì— ëŒ€í•´ ì›í•˜ëŠ” feature ê°¯ìˆ˜ë§Œ ì„ ì–¸ì„ í•˜ì˜€ê³ , ì…ë ¥ì˜ í¬ê¸°ëŠ” ì„ ì–¸í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. FlaxëŠ” ì´ëŸ¬í•œ ì„ ì–¸ì—ì„œ ì»¤ë„ì˜ ì…ë ¥ì— ëŒ€í•œ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì¤ë‹ˆë‹¤.
- ëœë¤ PRNG í‚¤ëŠ” ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. (í•´ë‹¹ ì˜ˆì œì—ì„œëŠ” ëª¨ë“ˆì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ê°’ì´ ìˆëŠ” ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.)
- ì´ˆê¸°í™” í•¨ìˆ˜ëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•  ì´ˆê¸°ì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ í˜¸ì¶œë©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” `(PRNG key, shape, dtype)`ì„ ì¸ìˆ˜ë¡œ ë°›ê³ , `shape`ë¥¼ Arrayë¡œ ë°˜í™¥ë‹ˆë‹¤.
- init í•¨ìˆ˜ëŠ” ì´ˆê¸°í™”ëœ ë§¤ê°œë³€ìˆ˜ ì„¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (`init` ëŒ€ì‹  `init_with_output` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ê°™ì€ ë¬¸ë²•ìœ¼ë¡œ ë”ë¯¸ ì…ë ¥ì— ëŒ€í•œ ì •ë°©í–¥ íŒ¨ìŠ¤ì˜ ì¶œë ¥ ë˜í•œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ë©´ parameterë“¤ì€ `FrozenDict` ì¸ìŠ¤í„´ìŠ¤ì— ì €ì¥ì´ ë©ë‹ˆë‹¤. ì´ëŠ” JAXì˜ í•¨ìˆ˜ì  íŠ¹ì„±ì„ ë‹¤ë£¨ê¸° ìœ„í•´ ë‚´ë¶€ì˜ dictì˜ ë³€ê²½ì„ ë°©ì§€í•˜ê³  ì‚¬ìš©ìê°€ ì´ëŸ¬í•œ ë³€ê²½ì„ ì¸ì§€í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ì´ì— ëŒ€í•´ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì„¸ìš”. **`[flax.core.frozen_dict.FrozenDict`**Â API docs](https://flax.readthedocs.io/en/latest/api_reference/flax.core.frozen_dict.html#flax.core.frozen_dict.FrozenDict).

ê²°ê³¼ì ìœ¼ë¡œ, ì•„ë˜ì˜ ì˜ˆì œëŠ” ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.


```python
try:
    params['new_key'] = jnp.ones((2,2))
except ValueError as e:
    print("Error: ", e)
```

    Error:  FrozenDict is immutable.


ì£¼ì–´ì§„ ë§¤ê°œë³€ìˆ˜ ì„¸íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì •ë°©í–¥ íŒ¨ìŠ¤ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” apply ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤ (ì•ì„œ ë§í–ˆë“¯ì´ ë§¤ê°œë³€ìˆ˜ë“¤ì€ ëª¨ë¸ê³¼ í•¨ê»˜ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.). apply ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ìš©í•  ë§¤ê°œë³€ìˆ˜ì™€ ì…ë ¥ ê°’ì„ ì œê³µí•˜ë©´ ë©ë‹ˆë‹¤.


```python
model.apply(params, x)
```




    Array([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 , -1.1271116 ],      dtype=float32)



## ê²½ì‚¬í•˜ê°•ë²•

JAX Partë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ë°”ë¡œ í•´ë‹¹ ë…¸íŠ¸ë¶ìœ¼ë¡œ ì™”ë‹¤ë©´, ì—¬ê¸°ì„œ ì‚¬ìš©í•  ì„ í˜• íšŒê·€ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

íŠ¹ì • ë°ì´í„° í¬ì¸í„° ì„¸íŠ¸ $\{(x_i, y_i), i \in {1,â€¦,k}, x_i \in R^n, y_i \in R^m\}$ë¡œë¶€í„°, í•¨ìˆ˜ $f_{W,b}(x) = Wx +b$ì˜ íŒŒë¼ë¯¸í„°ì„¸íŠ¸ $W \in M_{m,n}(R), b \in R^m$ë¥¼ ì°¾ê³ ì í•˜ë©° ì´ëŠ” í•´ë‹¹ í•¨ìˆ˜ì— ëŒ€í•œ mean squared errorë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

$$
L(W,b) \rightarrow \frac{1}{k}\sum^k_{i=1}\frac{1}{2}||y_i -f_{W,b}(x_i)||^2_2
$$

ì—¬ê¸°ì—ì„œ íŠœí”Œ ì´ dense layerì˜ ë§¤ê°œë³€ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ì´ìš©í•˜ì—¬ ê²½ì‚¬í•˜ê°•ë²•ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì‚¬ìš©í•  ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ ì‹¤ìŠµì„ ì§„í–‰í•´ë´…ì‹œë‹¤. í•´ë‹¹ ë°ì´í„°ëŠ” JAX íŒŒíŠ¸ì˜ linear regression pytree ì˜ˆì œì™€ ê°™ìŠµë‹ˆë‹¤.


```python
# ì°¨ì› ì„¤ì •
n_samples = 20
x_dim = 10
y_dim = 5

# ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” Wì™€ bë¥¼ ìƒì„±
key = random.PRNGKey(0)
k1, k2 = random.split(key)
W = random.normal(k1, (x_dim, y_dim))
b = random.normal(k2, (y_dim,))
# FrozenDict pytreeì— íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥
true_params = freeze({'params': {'bias': b, 'kernel': W}})

# ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ìƒ˜í”Œ ìƒì„±
key_sample, key_noise = random.split(k1)
x_samples = random.normal(key_sample, (n_samples, x_dim))
y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))
print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)
```

    x shape: (20, 10) ; y shape: (20, 5)


jax.value_and_grad()ë¥¼ ì´ìš©í•˜ì—¬ JAX pytree linear regression ì˜ˆì œì—ì„œ ìˆ˜í–‰í–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ í•™ìŠµ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, ì°¨ì´ì ì€ ì§ì ‘ ì •ì˜í•œ feed-forwad í•¨ìˆ˜ ëŒ€ì‹ ì— model.apply()ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (JAX exampleì˜ predict_pytree())


```python
# JAX ë²„ì „ê³¼ ê°™ì§€ë§Œ ì—¬ê¸°ì—ì„œëŠ” model.apply()ë¥¼ ì‚¬ìš©
@jax.jit
def mse(params, x_batched, y_batched):
  # (x,y) ìŒì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
  def squared_error(x, y):
    pred = model.apply(params, x)
    return jnp.inner(y-pred, y-pred) / 2.0
  # ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ ë²¡í„°í™”
  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ gradient descentë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.


```python
learning_rate = 0.3  # í•™ìŠµë¥ 
print('Loss for "true" W,b: ', mse(true_params, x_samples, y_samples))
loss_grad_fn = jax.value_and_grad(mse)

@jax.jit
def update_params(params, learning_rate, grads):
  params = jax.tree_util.tree_map(
      lambda p, g: p - learning_rate * g, params, grads)
  return params

for i in range(101):
  # ì—…ë°ì´íŠ¸ ìˆ˜í–‰
  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)
  params = update_params(params, learning_rate, grads)
  if i % 10 == 0:
    print(f'Loss step {i}: ', loss_val)
```

    Loss for "true" W,b:  0.02363979
    Loss step 0:  35.343876
    Loss step 10:  0.5143469
    Loss step 20:  0.11384159
    Loss step 30:  0.03932674
    Loss step 40:  0.01991621
    Loss step 50:  0.014209136
    Loss step 60:  0.012425653
    Loss step 70:  0.01185039
    Loss step 80:  0.011661786
    Loss step 90:  0.011599408
    Loss step 100:  0.011578696


## Optaxë¥¼ ì´ìš©í•œ ìµœì í™”

FlaxëŠ” ìµœì í™”ë¥¼ ìœ„í•´ Flaxì˜ `flax.optim` íŒ¨í‚¤ì§€ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ [FLIP #1009](https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md)ë¡œ ì¸í•´ [Optax](https://github.com/deepmind/optax)ê°€ ëŒ€ì‹  ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì´ íŒ¨í‚¤ì§€ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

Optaxì˜ ê¸°ë³¸ì ì€ ì‚¬ìš© ë°©ë²•ì€ ì§ê´€ì ì…ë‹ˆë‹¤:

1. ìµœì í™” ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤. (e.g. `optax.adam`)
2. íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ ìµœì í™” ìƒíƒœë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (Adamì˜ ê²½ìš°, í•´ë‹¹ ìƒíƒœëŠ” [momentum values](https://optax.readthedocs.io/en/latest/api.html#optax.adam)ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.)
3. ì†ì‹¤ì— ëŒ€í•´ì„œ `jax.value_and_grad()`ë¥¼ ì´ìš©í•˜ì—¬ gradientë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
4. ë§¤ ë°˜ë³µë§ˆë‹¤, Optaxì˜ `update` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë‚´ë¶€ì˜ ìµœì í™” ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ Optaxì˜ `apply_updates` ë©”ì„œë“œë¥¼ í†µí•´ ì—…ë°ì´íŠ¸ë¥¼ íŒŒë¼ë¯¸í„°ì— ë°˜ì˜í•©ë‹ˆë‹¤.

OptaxëŠ” ë” ë§ì€ ì¼ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ :  ê°„ë‹¨í•œ gradient ë³€í™˜ì„ ë” ë³µì¡í•œ ë³€í™˜ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë‹¤ì–‘í•œ ìµœì í™”ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‹œê°„ì— ë”°ë¼ ìµœì í™”ì— ì‚¬ìš©ë˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ëŠ” (â€ìŠ¤ì¼€ì¥´â€) ê¸°ëŠ¥ì„ ì§€ì›í•˜ë©°, ë§¤ê°œë³€ìˆ˜ íŠ¸ë¦¬ì˜ íŠ¹ì • ë¶€ë¶„ì— ëŒ€í•´ ë‹¤ë¥´ê²Œ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê¸°ëŠ¥ (â€ë§ˆìŠ¤í‚¹â€) ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤. [official documentation](https://optax.readthedocs.io/en/latest/).


```python
import optax
tx = optax.adam(learning_rate=learning_rate)
opt_state = tx.init(params)
loss_grad_fn = jax.value_and_grad(mse)
```


```python
for i in range(101):
  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)
  updates, opt_state = tx.update(grads, opt_state)
  params = optax.apply_updates(params, updates)
  if i % 10 == 0:
    print('Loss step {}: '.format(i), loss_val)
```

    Loss step 0:  0.011577628
    Loss step 10:  0.26143155
    Loss step 20:  0.07675027
    Loss step 30:  0.03644055
    Loss step 40:  0.022012806
    Loss step 50:  0.016178599
    Loss step 60:  0.013002801
    Loss step 70:  0.012026143
    Loss step 80:  0.011764514
    Loss step 90:  0.011646044
    Loss step 100:  0.011585529


## ê²°ê³¼ì˜ ì§ë ¬í™”

í•™ìŠµì˜ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ë‹¤ë©´, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§¤ê°œë³€ìˆ˜ë¥¼ ì €ì¥í•˜ê³ ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Flaxì—ì„œëŠ” ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” ì§ë ¬í™” íŒ¨í‚¤ì§€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


```python
from flax import serialization
bytes_output = serialization.to_bytes(params)
dict_output = serialization.to_state_dict(params)
print('Dict output')
print(dict_output)
print('Bytes output')
print(bytes_output)
```

    Dict output
    {'params': {'bias': Array([-1.4555768 , -2.0277991 ,  2.0790975 ,  1.2186145 , -0.99809754],      dtype=float32), 'kernel': Array([[ 1.0098814 ,  0.18934374,  0.04454996, -0.9280221 ,  0.3478402 ],
           [ 1.7298453 ,  0.9879368 ,  1.1640464 ,  1.1006076 , -0.10653935],
           [-1.2029463 ,  0.28635228,  1.4155979 ,  0.11870951, -1.3141483 ],
           [-1.1941489 , -0.18958491,  0.03413862,  1.3169426 ,  0.0806038 ],
           [ 0.1385241 ,  1.3713038 , -1.3187183 ,  0.53152674, -2.2404997 ],
           [ 0.56294024,  0.8122311 ,  0.3175201 ,  0.53455096,  0.9050039 ],
           [-0.37926027,  1.7410393 ,  1.0790287 , -0.5039833 ,  0.9283062 ],
           [ 0.9706492 , -1.3153403 ,  0.33681503,  0.8099344 , -1.2018458 ],
           [ 1.0194312 , -0.6202479 ,  1.0818833 , -1.838974  , -0.45805007],
           [-0.6436537 ,  0.45666698, -1.1329137 , -0.6853864 ,  0.16829035]],      dtype=float32)}}
    Bytes output
    b'\x81\xa6params\x82\xa4bias\xc7!\x01\x93\x91\x05\xa7float32\xc4\x14WP\xba\xbfv\xc7\x01\xc0\xef\x0f\x05@\x8f\xfb\x9b?R\x83\x7f\xbf\xa6kernel\xc7\xd6\x01\x93\x92\n\x05\xa7float32\xc4\xc8\xcbC\x81?S\xe3A>\x06z6=\xdb\x92m\xbf\x1c\x18\xb2>\x92k\xdd?m\xe9|?y\xff\x94?\xb6\xe0\x8c?M1\xda\xbd%\xfa\x99\xbf\xc4\x9c\x92>P2\xb5?\xf9\x1d\xf3=\x036\xa8\xbf\xdf\xd9\x98\xbf\x8c"B\xbe\xef\xd4\x0b=\x93\x91\xa8?\x9b\x13\xa5=C\xd9\r>\xe2\x86\xaf?\xc3\xcb\xa8\xbf#\x12\x08?Yd\x0f\xc0\xda\x1c\x10?a\xeeO?\xff\x91\xa2>U\xd8\x08?V\xaeg?g.\xc2\xbe`\xda\xde?\x9d\x1d\x8a?\r\x05\x01\xbfz\xa5m?w|x?\x12]\xa8\xbf\x05s\xac>\xdcWO?\x15\xd6\x99\xbf\xb9|\x82?\x91\xc8\x1e\xbf\'{\x8a?\x80c\xeb\xbf\x8a\x85\xea\xbe}\xc6$\xbfA\xd0\xe9>Q\x03\x91\xbf|u/\xbfNT,>'


ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ë ¤ë©´, ëª¨ë¸ ì´ˆê¸°í™” ì‹œ ì–»ì„ ìˆ˜ ìˆëŠ” ê²ƒê³¼ ê°™ì€ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ êµ¬ì¡° íƒ¬í”Œë¦¿ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì´ì „ì— ìƒì„±ëœ íŒŒë¼ë¯¸í„°ë“¤ì„ íƒ¬í”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ë³€ìˆ˜ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ë©°, ê¸°ì¡´ ë³€ìˆ˜ë¥¼ ì§ì ‘ ë³€ê²½í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.

íƒ¬í”Œë¦¿ì„ í†µí•´ êµ¬ì¡°ë¥¼ ê°•ì œí•˜ëŠ” ê²ƒì˜ ëª©ì ì€ ì‚¬ìš©ìì˜ ì´ìŠˆë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë¨¼ì € ë§¤ê°œë³€ìˆ˜ì˜ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ëŠ” ì •í™•í•œ ëª¨ë¸ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.


```python
serialization.from_bytes(params, bytes_output)
```




    FrozenDict({
        params: {
            bias: array([-1.4555768 , -2.0277991 ,  2.0790975 ,  1.2186145 , -0.99809754],
                  dtype=float32),
            kernel: array([[ 1.0098814 ,  0.18934374,  0.04454996, -0.9280221 ,  0.3478402 ],
                   [ 1.7298453 ,  0.9879368 ,  1.1640464 ,  1.1006076 , -0.10653935],
                   [-1.2029463 ,  0.28635228,  1.4155979 ,  0.11870951, -1.3141483 ],
                   [-1.1941489 , -0.18958491,  0.03413862,  1.3169426 ,  0.0806038 ],
                   [ 0.1385241 ,  1.3713038 , -1.3187183 ,  0.53152674, -2.2404997 ],
                   [ 0.56294024,  0.8122311 ,  0.3175201 ,  0.53455096,  0.9050039 ],
                   [-0.37926027,  1.7410393 ,  1.0790287 , -0.5039833 ,  0.9283062 ],
                   [ 0.9706492 , -1.3153403 ,  0.33681503,  0.8099344 , -1.2018458 ],
                   [ 1.0194312 , -0.6202479 ,  1.0818833 , -1.838974  , -0.45805007],
                   [-0.6436537 ,  0.45666698, -1.1329137 , -0.6853864 ,  0.16829035]],
                  dtype=float32),
        },
    })



## ì‚¬ìš©ì ëª¨ë¸ ì •ì˜í•˜ê¸°

FlaxëŠ” ë‹¨ìˆœ ì„ í˜• íšŒê·€ë³´ë‹¤ ë³µì¡í•œ ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ì •ì˜í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. í•´ë‹¹ ì„¹ì…˜ì—ì„œëŠ” ê°„ë‹¨í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `nn.Module` í´ë˜ìŠ¤ì˜ ì„œë¸Œ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ê¸°ì–µí•´ì•¼ í•  ê²ƒì€ ìœ„ì—ì„œ `linen as nn`ì„ ì„ ì–¸í•˜ì˜€ê³  ì´ëŠ” ìƒˆë¡œìš´ linen APIì™€ í•¨ê»˜ ì‘ë™í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

### ê¸°ë³¸ ëª¨ë“ˆ

ëª¨ë¸ì˜ ê¸°ë³¸ ì¶”ìƒí™”ëŠ” `nn.Module` í´ë˜ìŠ¤ì´ë©°, Flaxì˜ ë¯¸ë¦¬ ì •ì˜ëœ ë ˆì´ì–´(ì´ì „ì— ì‚¬ìš©í•œ `Dense`ì™€ ê°™ì€)ì˜ ê° ìœ í˜•ì€ `nn.Module`ì˜ í•˜ìœ„ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ê°„ë‹¨í•˜ì§€ë§Œ ì‚¬ìš©ì ì •ì˜ì˜ ë‹¤ì¤‘ í¼ì…‰íŠ¸ë¡ ì¸ Dense ë ˆì´ì–´ì™€ ë¹„ì„ í˜• í•¨ìˆ˜ê°€ ë²ˆê°•í•˜ ë‚˜ì˜¤ëŠ” ì‹œí€€ìŠ¤ë¥¼ ì •ì˜í•˜ê³  ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.


```python
class ExplicitMLP(nn.Module):
  features: Sequence[int]

  def setup(self):
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ self.layersë¥¼ ì‘ì„±
    self.layers = [nn.Dense(feat) for feat in self.features]
    # í•˜ë‚˜ì˜ í•˜ìœ„ ëª¨ë“ˆë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì•„ë˜ì™€ ê°™ì´ ì‘ì„±í•©ë‹ˆë‹¤.
    # self.layer1 = nn.Dense(feat1)

  def __call__(self, inputs):
    x = inputs
    for i, lyr in enumerate(self.layers):
      x = lyr(x)
      if i != len(self.layers) - 1:
        x = nn.relu(x)
    return x

key1, key2 = random.split(random.PRNGKey(0), 2)
x = random.uniform(key1, (4,4))

model = ExplicitMLP(features=[3,4,5])
params = model.init(key2, x)
y = model.apply(params, x)

print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))
print('output:\n', y)
```

    initialized parameter shapes:
     {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}
    output:
     [[ 0.          0.          0.          0.          0.        ]
     [ 0.0072379  -0.00810348 -0.0255094   0.02151717 -0.01261241]
     [ 0.          0.          0.          0.          0.        ]
     [ 0.          0.          0.          0.          0.        ]]


ë³´ì‹œë‹¤ì‹œí”¼, nn.Moduleì˜ í•˜ìœ„ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. :

- ë°ì´í„° í•„ë“œì˜ ì§‘í•© (nn.Moduleì€ Python ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.) - ì—¬ê¸°ì„œëŠ” Sequence[int] ìœ í˜•ì˜ feature í•„ë“œë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- `__postinit__`ì˜ ëì—ì„œ í˜¸ì¶œë˜ëŠ” setup() ë©”ì„œë“œê°€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ ëª¨ë¸ì— í•„ìš”í•œ í•˜ìœ„ ëª¨ë“ˆ, ë³€ìˆ˜, ë§¤ê°œë³€ìˆ˜ë¥¼ ì„ ì–¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `__call__` í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì…ë ¥ìœ¼ë¡œë¶€í„° ëª¨ë¸ì˜ ì¶œë ¥ì„ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
- ëª¨ë¸ êµ¬ì¡°ëŠ” ëª¨ë¸ê³¼ ë™ì¼í•œ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë”°ë¼ì„œ ë§¤ê°œë³€ìˆ˜ì˜ pytreeë¥¼ ì •ì˜í•©ë‹ˆë‹¤. paramsì—ëŠ” ë ˆì´ì–´ ë‹¹ í•˜ë‚˜ì˜ `layers_n` ì˜ í•˜ìœ„ dictionaryê°€ ìˆê³ , ê°ê°ì—ëŠ” í•´ë‹¹ Dense ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë ˆì´ì•„ì›ƒì€ ë§¤ìš° ëª…ì‹œì ì…ë‹ˆë‹¤.

ì°¸ê³  : ëŒ€ë¶€ë¶„ì€ ì˜ˆìƒí•œ ëŒ€ë¡œ ê´€ë¦¬ë˜ì§€ë§Œ, ì—¬ê¸°[here](https://github.com/google/flax/issues/524)ì—ì„œ ì–¸ê¸‰ëœ ëŒ€ë¡œ ì•Œê³  ìˆì–´ì•¼ í•  ì½”ë„ˆ ì¼€ì´ìŠ¤ë“¤ì´ ìˆìŠµë‹ˆë‹¤.

ëª¨ë“ˆ êµ¬ì¡°ì™€ í•´ë‹¹ ë§¤ê°œë³€ìˆ˜ëŠ” ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ì§ì ‘ model(x)ë¥¼ í˜¸ì¶œ í•  ìˆ˜ëŠ” ì—†ê³  í˜¸ì¶œí•˜ê²Œ ë˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. `__call__`ì€ apply í•¨ìˆ˜ë¡œ ìŒ“ì—¬ìˆìœ¼ë©°, ì´ í•¨ìˆ˜ë¥¼ ì…ë ¥ì— ëŒ€í•´ í˜¸ì¶œ í•´ì•¼ í•©ë‹ˆë‹¤.


```python
try:
    y = model(x) # ì—ëŸ¬ë¥¼ ë°˜í™˜
except AttributeError as e:
    print(e)
```

    "ExplicitMLP" object has no attribute "layers". If "layers" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.


ì´ë²ˆ ì˜ˆì œëŠ” ë§¤ì£¼ ê°„ë‹¨í•œ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì—, `@nn.compact` ì–´ë…¸í…Œì´ì…˜ì„ ì‚¬ìš©í•˜ì—¬ `__call__` ë‚´ì—ì„œ í•˜ìœ„ ëª¨ë“ˆì„ ì¸ë¼ì¸ìœ¼ë¡œ ì„ ì–¸í•˜ëŠ” ëŒ€ì•ˆì ì¸ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. :


```python
class SimpleMLP(nn.Module):
  features: Sequence[int]

  @nn.compact
  def __call__(self, inputs):
    x = inputs
    for i, feat in enumerate(self.features):
      x = nn.Dense(feat, name=f'layers_{i}')(x)
      if i != len(self.features) - 1:
        x = nn.relu(x)
      # layerì— ì´ë¦„ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
      # ê¸°ë³¸ ì´ë¦„ì€ "Dense_0", "Dense_1", ...ì™€ ê°™ì´ ì§€ì •ë©ë‹ˆë‹¤.
    return x

key1, key2 = random.split(random.PRNGKey(0), 2)
x = random.uniform(key1, (4,4))

model = SimpleMLP(features=[3,4,5])
params = model.init(key2, x)
y = model.apply(params, x)

print('initialized parameter shapes:\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))
print('output:\n', y)
```

    initialized parameter shapes:
     {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}
    output:
     [[ 0.          0.          0.          0.          0.        ]
     [ 0.0072379  -0.00810348 -0.0255094   0.02151717 -0.01261241]
     [ 0.          0.          0.          0.          0.        ]
     [ 0.          0.          0.          0.          0.        ]]


ê·¸ëŸ¬ë‚˜ ë‘ ê°€ì§€ ëª¨ë“œ ì‚¬ì´ì— ì•Œì•„ë‘¬ì•¼ í•  ëª‡ ê°€ì§€ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤:

- `setup`ì„ ì´ìš©í•˜ë©´ ì¼ë¶€ í•˜ìœ„ ë ˆì´ì–´ì— ì´ë¦„ì„ ì§€ì •í•˜ê³  ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: ì˜¤í† ì¸ì½”ë”ì˜ ì¸ì½”ë”/ë””ì½”ë” ë©”ì„œë“œ).
- ì—¬ëŸ¬ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `@nn.compact` ì–´ë…¸í…Œì´ì…˜ ëŒ€ì‹  `setup`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“ˆì„ ì„ ì–¸í•´ì•¼ í•©ë‹ˆë‹¤. `@nn.compact` ì–´ë…¸í…Œì´ì…˜ì€ í•˜ë‚˜ì˜ ë©”ì„œë“œë§Œ ì–´ë…¸í…Œì´ì…˜ìœ¼ë¡œ í—ˆìš©í•©ë‹ˆë‹¤.
- ë§ˆì§€ë§‰ ì´ˆê¸°í™”ëŠ” ë‹¤ë¥´ê²Œ ì²˜ë¦¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì´ ë…¸íŠ¸ë¥¼ ì°¸ì¡°í•˜ì„¸ìš” (TODO: ë…¸íŠ¸ ë§í¬ ì¶”ê°€ ì˜ˆì •).

## ëª¨ë“ˆ íŒŒë¼ë¯¸í„°

ì´ì „ MLP ì˜ˆì œì—ì„œëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ë ˆì´ì–´ì™€ ì—°ì‚°ì (`Dense`, `relu`)ë§Œì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Flaxì—ì„œ ì œê³µí•˜ëŠ” Dense ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ ì§ì ‘ ì‘ì„±í•˜ê³ ì í•œë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ `@nn.compact` ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë“ˆì„ ì„ ì–¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```python
class SimpleDense(nn.Module):
  features: int
  kernel_init: Callable = nn.initializers.lecun_normal()
  bias_init: Callable = nn.initializers.zeros_init()

  @nn.compact
  def __call__(self, inputs):
    kernel = self.param('kernel',
                        self.kernel_init, # ì´ˆê¸°í™” í•¨ìˆ˜
                        (inputs.shape[-1], self.features))  # í˜•íƒœ ì •ë³´
    y = lax.dot_general(inputs, kernel,
                        (((inputs.ndim - 1,), (0,)), ((), ())),)
    bias = self.param('bias', self.bias_init, (self.features,))
    y = y + bias
    return y

key1, key2 = random.split(random.PRNGKey(0), 2)
x = random.uniform(key1, (4,4))

model = SimpleDense(features=3)
params = model.init(key2, x)
y = model.apply(params, x)

print('initialized parameters:\n', params)
print('output:\n', y)
```

    initialized parameters:
     FrozenDict({
        params: {
            kernel: Array([[ 0.61506   , -0.22728713,  0.6054702 ],
                   [-0.29617992,  1.1232013 , -0.879759  ],
                   [-0.35162622,  0.3806491 ,  0.6893246 ],
                   [-0.1151355 ,  0.04567898, -1.091212  ]], dtype=float32),
            bias: Array([0., 0., 0.], dtype=float32),
        },
    })
    output:
     [[-0.02996203  1.102088   -0.6660265 ]
     [-0.31092793  0.63239413 -0.53678817]
     [ 0.01424009  0.9424717  -0.63561463]
     [ 0.3681896   0.3586519  -0.00459218]]


ì—¬ê¸°ì„œëŠ” `self.param` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì— ë§¤ê°œë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ê³  í• ë‹¹í•˜ëŠ” ë°©ë²•ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” `(name, init_fn, *init_args)`ë¥¼ ì¸ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

- `name`ì€ ë§¤ê°œë³€ìˆ˜ì˜ ì´ë¦„ì´ë©°, ë§¤ê°œë³€ìˆ˜ êµ¬ì¡°ì— ì €ì¥ë©ë‹ˆë‹¤.
- `init_fn`ì€ `(PRNGKey, *init_args)`ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Arrayë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë©°, `init_args`ëŠ” ì´ˆê¸°í™” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë° í•„ìš”í•œ ì¸ìˆ˜ì…ë‹ˆë‹¤.
- `init_args`ëŠ” ì´ˆê¸°í™” í•¨ìˆ˜ì— ì œê³µí•´ì•¼ í•˜ëŠ” ì¸ìˆ˜ì…ë‹ˆë‹¤.

ì´ëŸ¬í•œ ë§¤ê°œë³€ìˆ˜ëŠ” `setup` ë©”ì„œë“œì—ì„œë„ ì„ ì–¸í•  ìˆ˜ ìˆìœ¼ë©°, FlaxëŠ” ì²« ë²ˆì§¸ í˜¸ì¶œ ì§€ì ì—ì„œ ì§€ì—° ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í˜•ìƒ ì¶”ë¡ ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

## ë³€ìˆ˜ì™€ ë³€ìˆ˜ë“¤ì˜ ì§‘í•©

ì§€ê¸ˆê¹Œì§€ ë³¸ ë°”ì™€ ê°™ì´, ëª¨ë¸ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤:

- `nn.Module`ì˜ í•˜ìœ„ í´ë˜ìŠ¤
- ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ pytree (ì¼ë°˜ì ìœ¼ë¡œ `model.init()`ì„ ìˆ˜í–‰í•˜ì—¬ ì–»ìŒ)

ê·¸ëŸ¬ë‚˜ ì´ê²ƒìœ¼ë¡œëŠ” ë¨¸ì‹ ëŸ¬ë‹, íŠ¹íˆ ì‹ ê²½ë§ì— í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ë‹¤ë£¨ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¼ë¶€ ê²½ìš°ì—ëŠ” ì‹ ê²½ë§ì´ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ì¼ë¶€ ë‚´ë¶€ ìƒíƒœë¥¼ ì¶”ì í•˜ê³ ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: ë°°ì¹˜ ì •ê·œí™” ë ˆì´ì–´). `variable` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ ì´ì™¸ì˜ ë³€ìˆ˜ë¥¼ ì„ ì–¸í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.

ë°ëª¨ ëª©ì ìœ¼ë¡œ ë°°ì¹˜ ì •ê·œí™”ì™€ ìœ ì‚¬í•œ ë‹¨ìˆœí™”ëœ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•´ ë³´ê² ìŠµë‹ˆë‹¤: ì‹¤í–‰ í‰ê· ì„ ì €ì¥í•˜ê³  í•™ìŠµ ì‹œì— ì…ë ¥ì—ì„œ ì´ë¥¼ ë¹¼ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì‹¤ì œ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ (êµ¬í˜„ ë‚´ìš©ì„ ì‚´í´ë³´ë ¤ë©´) ì—¬ê¸°([here](https://github.com/google/flax/blob/main/flax/linen/normalization.py))ì—ì„œ ì œê³µí•˜ëŠ” êµ¬í˜„ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.


```python
class BiasAdderWithRunningMean(nn.Module):
  decay: float = 0.99

  @nn.compact
  def __call__(self, x):
    is_initialized = self.has_variable('batch_stats', 'mean')
    ra_mean = self.variable('batch_stats', 'mean',
                            lambda s: jnp.zeros(s),
                            x.shape[1:])
    mean = ra_mean.value
    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])
    if is_initialized:
      ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)

    return x - ra_mean.value + bias


key1, key2 = random.split(random.PRNGKey(0), 2)
x = jnp.ones((10,5))
model = BiasAdderWithRunningMean()
variables = model.init(key1, x)
print('initialized variables:\n', variables)
y, updated_state = model.apply(variables, x, mutable=['batch_stats'])
print('updated state:\n', updated_state)
```

    initialized variables:
     FrozenDict({
        batch_stats: {
            mean: Array([0., 0., 0., 0., 0.], dtype=float32),
        },
        params: {
            bias: Array([0., 0., 0., 0., 0.], dtype=float32),
        },
    })
    updated state:
     FrozenDict({
        batch_stats: {
            mean: Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32),
        },
    })


ì—¬ê¸°ì„œ updated_stateëŠ” ëª¨ë¸ì´ ë°ì´í„°ì— ì ìš©ë  ë•Œ ë³€ê²½ë˜ëŠ” ìƒíƒœ ë³€ìˆ˜ë§Œì„ ë°˜í™˜í•©ë‹ˆë‹¤. ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ëª¨ë¸ì˜ ìƒˆë¡œìš´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì–»ìœ¼ë ¤ë©´ ë‹¤ìŒ íŒ¨í„´ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```python
for val in [1.0, 2.0, 3.0]:
  x = val * jnp.ones((10,5))
  y, updated_state = model.apply(variables, x, mutable=['batch_stats'])
  old_state, params = variables.pop('params')
  variables = freeze({'params': params, **updated_state})
  print('updated state:\n', updated_state) # mutable ë¶€ë¶„ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
```

    updated state:
     FrozenDict({
        batch_stats: {
            mean: Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32),
        },
    })
    updated state:
     FrozenDict({
        batch_stats: {
            mean: Array([[0.0299, 0.0299, 0.0299, 0.0299, 0.0299]], dtype=float32),
        },
    })
    updated state:
     FrozenDict({
        batch_stats: {
            mean: Array([[0.059601, 0.059601, 0.059601, 0.059601, 0.059601]], dtype=float32),
        },
    })


ì´ ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì „ì²´ì ì¸ BatchNorm êµ¬í˜„ ë˜ëŠ” ìƒíƒœë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  ë ˆì´ì–´ë¥¼ ìœ ë„í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì˜µí‹°ë§ˆì´ì €ì™€ ìƒíƒœ ë³€ìˆ˜ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ê¸° ìœ„í•´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¶”ê°€í•´ ë³´ê² ìŠµë‹ˆë‹¤.

ì´ ì˜ˆì œëŠ” ì•„ë¬´ ì‘ì—…ë„ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë©°, ë‹¨ì§€ ë°ëª¨ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.


```python
from functools import partial

@partial(jax.jit, static_argnums=(0, 1))
def update_step(tx, apply_fn, x, opt_state, params, state):

  def loss(params):
    y, updated_state = apply_fn({'params': params, **state},
                                x, mutable=list(state.keys()))
    l = ((x - y) ** 2).sum()
    return l, updated_state

  (l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)
  updates, opt_state = tx.update(grads, opt_state)
  params = optax.apply_updates(params, updates)
  return opt_state, params, state

x = jnp.ones((10,5))
variables = model.init(random.PRNGKey(0), x)
state, params = variables.pop('params')
del variables
tx = optax.sgd(learning_rate=0.02)
opt_state = tx.init(params)

for _ in range(3):
  opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)
  print('Updated state: ', state)
```

    Updated state:  FrozenDict({
        batch_stats: {
            mean: Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32),
        },
    })
    Updated state:  FrozenDict({
        batch_stats: {
            mean: Array([[0.0199, 0.0199, 0.0199, 0.0199, 0.0199]], dtype=float32),
        },
    })
    Updated state:  FrozenDict({
        batch_stats: {
            mean: Array([[0.029701, 0.029701, 0.029701, 0.029701, 0.029701]], dtype=float32),
        },
    })


ìœ„ í•¨ìˆ˜ëŠ” ê½¤ ìƒì„¸í•œ ì‹œê·¸ë‹ˆì²˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì‹¤ì œë¡œëŠ” jax.jit()ì™€ í•¨ê»˜ ì‘ë™í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” í•¨ìˆ˜ ì¸ìˆ˜ê°€ "ìœ íš¨í•œ JAX íƒ€ì…"ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

FlaxëŠ” ìœ„ì˜ ì½”ë“œë¥¼ ë‹¨ìˆœí™”í•˜ëŠ” ìœ ìš©í•œ ë˜í¼ì¸ `TrainState`ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [flax.training.train_state.TrainState](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState)ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.

## jax2tfë¥¼ ì´ìš©í•˜ì—¬ Tensorflowì˜ SavedModelë¡œ í¬íŒ…í•˜ê¸°

[jax2tf](https://github.com/google/jax/tree/main/jax/experimental/jax2tf)ë¥¼ ì‚¬ìš©í•˜ë©´ í›ˆë ¨ëœ Flax ëª¨ë¸ì„ Tensorflowì˜ SavedModel í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ë³€í™˜ëœ ëª¨ë¸ì€ [TF Hub](https://www.tensorflow.org/hub),Â [TF.lite](https://www.tensorflow.org/lite),Â [TF.js](https://www.tensorflow.org/js) ë˜ëŠ” ë‹¤ë¥¸ í•˜í–¥ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ì €ì¥ì†Œì—ëŠ” Flaxì— ëŒ€í•œ ìì„¸í•œ ë¬¸ì„œì™€ ë‹¤ì–‘í•œ ì˜ˆì œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
